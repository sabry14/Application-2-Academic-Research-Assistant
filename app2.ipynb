{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO2yxJMpKpKW"
      },
      "outputs": [],
      "source": [
        "# Academic Research Assistant System\n",
        "#!pip install -U langchain-community arxiv wikipedia langchain-openai faiss-cpu sentence-transformers\n",
        "\n",
        "import os\n",
        "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI\n",
        "from typing import List, Dict\n",
        "import json\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# Initialize tools\n",
        "arxiv = ArxivAPIWrapper()\n",
        "wikipedia = WikipediaAPIWrapper()\n",
        "\n",
        "class AcademicResearchAssistant:\n",
        "    def __init__(self):\n",
        "        self.llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.3, openai_api_key=\"your_openai_api_key\")\n",
        "        self.vectorstore = None\n",
        "        self.sources = []\n",
        "\n",
        "    def research(self, query: str, tools: List[str] = [\"arxiv\", \"wikipedia\"]) -> Dict:\n",
        "        \"\"\"Collect research from specified tools\"\"\"\n",
        "        results = {}\n",
        "\n",
        "        if \"arxiv\" in tools:\n",
        "            results[\"arxiv\"] = arxiv.run(query)\n",
        "            self._add_sources(\"arxiv\", query)\n",
        "\n",
        "        if \"wikipedia\" in tools:\n",
        "            results[\"wikipedia\"] = wikipedia.run(query)\n",
        "            self._add_sources(\"wikipedia\", query)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _add_sources(self, tool: str, query: str):\n",
        "        \"\"\"Track sources for citations\"\"\"\n",
        "        self.sources.append({\n",
        "            \"tool\": tool,\n",
        "            \"query\": query,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        })\n",
        "\n",
        "    def synthesize(self, research_data: Dict, format: str = \"APA\") -> str:\n",
        "        \"\"\"Synthesize information with proper citations\"\"\"\n",
        "        template = f\"\"\"As an academic research assistant, synthesize this information into a {format}-formatted report:\n",
        "\n",
        "        Research Data:\n",
        "        {{research}}\n",
        "\n",
        "        Include in-text citations and a references section. Use {format} citation style.\"\"\"\n",
        "\n",
        "        prompt = ChatPromptTemplate.from_template(template)\n",
        "        chain = prompt | self.llm | StrOutputParser()\n",
        "\n",
        "        return chain.invoke({\"research\": json.dumps(research_data)})\n",
        "\n",
        "    def filter_relevant(self, documents: List[str], query: str, threshold: float = 0.7) -> List[str]:\n",
        "        \"\"\"Filter documents by relevance score\"\"\"\n",
        "        if not self.vectorstore:\n",
        "            self._create_vectorstore(documents)\n",
        "\n",
        "        docs = self.vectorstore.similarity_search_with_relevance_scores(query)\n",
        "        return [doc[0].page_content for doc in docs if doc[1] > threshold]\n",
        "\n",
        "    def _create_vectorstore(self, documents: List[str]):\n",
        "        \"\"\"Create vector store for documents\"\"\"\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "        splits = text_splitter.create_documents(documents)\n",
        "        embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "        self.vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "\n",
        "# Example Usage\n",
        "if __name__ == \"__main__\":\n",
        "    assistant = AcademicResearchAssistant()\n",
        "\n",
        "    # 1. Conduct research\n",
        "    research_data = assistant.research(\n",
        "        \"quantum computing applications in cryptography\",\n",
        "        tools=[\"arxiv\", \"wikipedia\"]\n",
        "    )\n",
        "\n",
        "    # 2. Filter relevant info\n",
        "    documents = [research_data[\"arxiv\"], research_data[\"wikipedia\"]]\n",
        "    relevant_docs = assistant.filter_relevant(\n",
        "        documents,\n",
        "        \"post-quantum cryptography algorithms\",\n",
        "        threshold=0.65\n",
        "    )\n",
        "\n",
        "    # 3. Generate formatted report\n",
        "    report = assistant.synthesize(\n",
        "        {\"relevant_docs\": relevant_docs},\n",
        "        format=\"APA\"\n",
        "    )\n",
        "\n",
        "    print(\"# Academic Research Report\\n\")\n",
        "    print(report)\n",
        "\n",
        "    print(\"\\n## Research Sources\\n\")\n",
        "    for i, source in enumerate(assistant.sources, 1):\n",
        "        print(f\"{i}. {source['tool'].title()}: {source['query']}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PiNtM_uiNjIx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}